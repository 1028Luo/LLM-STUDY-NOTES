{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOt23lSdNzeW7UPjXfL/9yS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=zy8ChVd_oTM"
      ],
      "metadata": {
        "id": "sNx8wKcct1AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax (safe, online)"
      ],
      "metadata": {
        "id": "dBpQM6_m-ES0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical instability"
      ],
      "metadata": {
        "id": "MMQ7UiO4-IbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block matrix multiplication"
      ],
      "metadata": {
        "id": "Q76g7uhXLRo4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l120ai3fnpaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flash Attention Forward"
      ],
      "metadata": {
        "id": "AxwJZCw5LUiD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kr_n-aPcfG-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallel hierarchy:\n",
        "\n",
        "Batch size X Num heads (multihead) X Blocks of Q (Seq_length / block_size)"
      ],
      "metadata": {
        "id": "2oxzlgkUqP9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to CUDA"
      ],
      "metadata": {
        "id": "oSdOgWdzfHWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector adding, kernel, indexingm, block"
      ],
      "metadata": {
        "id": "KsKXays3fKpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic unit of a GPU is 1 control unit + a bunch of cores/threads (also called a warp)\n",
        "\n",
        "Since there is only 1 control unit, all threads can only do the same computation\n",
        "\n",
        "Say we want to do vector addition with a vector of length 64, and our GPU only has 16 threads, we can divide the data into blocks, each with size 2, 4, 8 or 16. This way the GPU can process one or more blocks at a time.\n",
        "\n",
        "A kernel is how one control unit computes the data?"
      ],
      "metadata": {
        "id": "Cxq2m4mqd7mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Tensor"
      ],
      "metadata": {
        "id": "q8tym-whkXp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stride, matrix, transpose\n",
        "\n",
        "Matrices are stored as tensors, which essentially is [1, 2, 3, 4]\n",
        "\n",
        "Stride is the method for accessing them. For example, a stride of [2,1 ] means starting from the third variable, with a step 1, giving [3, 4] of the matrix.\n",
        "\n",
        "Other matrix operations such as transpose are also done in the same way[link text](https://)"
      ],
      "metadata": {
        "id": "t_i--wYkXrLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Triton"
      ],
      "metadata": {
        "id": "qgIbYG6Hkl4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Triton add_vector kernel implementation\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def add_kernel(\n",
        "\n",
        "    # No access to the whole vector,\n",
        "    # only *Pointer* to the start\n",
        "    x_ptr,  # first input vector.\n",
        "    y_ptr,  # second input vector.\n",
        "    output_ptr,  # output vector.\n",
        "\n",
        "    n_elements,  # Size of the vector.\n",
        "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
        "):\n",
        "    # NOTE: `constexpr` so it can be used as a shape value.\n",
        "\n",
        "    # There are multiple \"programs\" processing different data. We identify which program we are here:\n",
        "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
        "\n",
        "    # This program will process inputs that are offset from the initial data.\n",
        "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
        "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
        "    # Note that offsets is a list of pointers:\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # 2048, 2049, ... 2060, 2061, 2062...\n",
        "\n",
        "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
        "    mask = offsets < n_elements\n",
        "\n",
        "    # Load x and y from DRAM, masking out any extra elements in case the input is not a multiple of the block size.\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    y = tl.load(y_ptr + offsets, mask=mask)\n",
        "\n",
        "    output = x + y\n",
        "\n",
        "    # Write x + y back to DRAM.\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n"
      ],
      "metadata": {
        "id": "E_Cod3Dikq8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flash Attention forward"
      ],
      "metadata": {
        "id": "nXVCB7uonX6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "iC0VtwNEnZ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every operation implemented with pytorch, such as softmax, relu\n",
        "# is derived from torch.autograd.Function\n",
        "# Each implementation should contain forward pass and backward pass\n",
        "class TritonAttention(torch.autograd.Function):\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, Q, K, V, causal, softmax_scale):\n",
        "\n",
        "    # check size\n",
        "    HEAD_DIM_Q, HEAD_DIM_K, HEAD_DIM_V = Q.shape[-1], K.shape[-1]\n",
        "    HEAD_DIM_V = V.shape[-1]\n",
        "\n",
        "    BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
        "\n",
        "    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2DsNriiuvUrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}